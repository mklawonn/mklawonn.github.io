---
title: "Word Frequency Analysis in Orbital Inflammation Literature"
format:
  html:
    code-fold: true
jupyter: python3
---

```{python}
#| echo: false
from pathlib import Path
pub_dir = Path.cwd() / "orbital_publications"
f_list = sorted([x for x in pub_dir.glob("*.pdf")])
years = [2005, 2007, 2010, 2016, 2024, 2016,
         2020, 2022, 2016, 2017, 2017, 2019,
         2024, 2004, 2014, 2021, 2015, 2007,
         2011, 2017, 2024, 2015, 2023, 2022,
         2020, 2021, 2015, 2024, 2023,
         2021, 2013, 2023, 2012, 2022, 2024,
         2008, 2019, 2015, 2011, 2016, 2019,
         2007, 2006, 2007, 2022, 2022, 2022,
         2014, 2012, 2018, 2021, 2022, 2017,
         2009, 2013, 2013, 2012, 2014, 2022,
         2006, 2016, 2017, 2011, 2008, 2006,
         2014, 2012, 2009, 2010, 2016, 2015,
         2018, 2016, 2014, 2023, 2018, 2014,
         2008, 2024, 2014, 2013, 2022, 2024,
         2011, 2021, 2023, 2014, 2024, 2023,
         2014, 2019, 2017, 2015, 2013, 2013,
         2015, 2022, 2009, 2012, 2014, 2024,
         2021, 2020, 2022, 2024, 2018, 2017,
         2021, 2008, 2012, 2010, 2008, 2021,
         2019, 2013, 2018, 2018, 2017, 2007,
         2012, 2011, 2008, 2017, 2017, 2009,
         2024, 2013]
assert len(f_list) == len(years)
files = {2005 : [], 2010 : [], 2015 : [], 2020 : []}
for i, f in enumerate(f_list):
  if years[i] < 2010:
    files[2005].append(f)
  elif years[i] < 2015:
    files[2010].append(f)
  elif years[i] < 2020:
    files[2015].append(f)
  else:
    files[2020].append(f)
```

```{python}
#| echo: false
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_medllama_3():
    tokenizer = AutoTokenizer.from_pretrained("Henrychur/MMed-Llama-3-8B")
    return tokenizer
```

```{python}
#| echo: false
import spacy
from tokenizers import normalizers, NormalizedString
from tokenizers.normalizers import NFD, StripAccents, NFC
from tokenizers.pre_tokenizers import Whitespace
import string


def filter_text(t, tokenizer):
    import unicodedata
    nlp = spacy.load("en_core_web_sm")
    #nlp = spacy.load("en_core_web_trf")
    stopwords = []
    #Stopwords from: https://www.oocities.org/gumby9/physicians/advanced/stopwords.pdf
    with open("pubmed_stopwords.txt", "r") as f:
        for line in f:
            stopwords.append(line.strip())
    with open("custom_stopwords.txt", "r") as f:
        for line in f:
            stopwords.append(line.strip())
    stopwords = set(stopwords)
    normalizer = normalizers.Sequence([NFD(), StripAccents()])
    t = normalizer.normalize_str(t)
    t = unicodedata.normalize("NFKD", t)
    t = "".join([c for c in t if not unicodedata.combining(c)])
    pre_tokenizer = Whitespace()
    #t = normalizer.normalize(NormalizedString(t))
    #print(t)
    #I'm not sure if this will actually do anything. Maybe filter unknown tokens?
    #In any case lets give it a chance
    t = tokenizer.decode(tokenizer.encode(t), skip_special_tokens=True, clean_up_tokenization_spaces=True)
    t = t.translate(str.maketrans('', '', string.punctuation)).lower()
    t = pre_tokenizer.pre_tokenize_str(t)
    j = " ".join([w[0] for w in t])
    doc = nlp(j)
    #Remove stopwords
    return [toke.lemma_ for toke in doc if toke.lemma_ not in stopwords and not toke.lemma_.isnumeric()]
    #return [str(toke) for toke in doc if str(toke) not in stopwords and not toke.lemma_.isnumeric()]
```

```{python}
#| echo: false
from collections import Counter
from pathlib import Path

def count_differences(year_list):
    results = []
    sorted_years = sorted(year_list.keys())

    for i in range(len(sorted_years)):
        if i == len(sorted_years) - 1:
          y1 = sorted_years[0]
          y2 = sorted_years[i]
        else:
          y1, y2 = sorted_years[i], sorted_years[i+1]
        vocab1, vocab2 = year_list[y1], year_list[y2]
        y1_norm = sum(vocab1.values())
        y2_norm = sum(vocab2.values())
        if y1_norm == 0 or y2_norm == 0:
            continue

        top_50_y1 = [word for word, _ in Counter(vocab1).most_common(50)]
        sorted_y1_words = sorted(top_50_y1, key=lambda word: vocab2.get(word, 0))
        y1_diffs = sorted([(w, (vocab2.get(w, 0)/y2_norm) - (vocab1.get(w, 0)/y1_norm)) for w in sorted_y1_words], key=lambda p: p[1])

        top_50_y2 = [word for word, _ in Counter(vocab2).most_common(50)]
        sorted_y2_words = sorted(top_50_y2, key=lambda word: vocab1.get(word, 0))
        y2_diffs = sorted([(w, (vocab2.get(w, 0)/y2_norm) - (vocab1.get(w, 0)/y1_norm)) for w in sorted_y2_words], key=lambda p: p[1], reverse=True)

        decrease_table = [
            {"Year1": y1, "Year2": y2, "Rank": rank+1, "Word": word, "Diff": round(diff*100, 3), "Sign": "+" if diff > 0 else "-" if diff < 0 else "="}
            for rank, (word, diff) in enumerate(y1_diffs)
        ]

        increase_table = [
            {"Year1": y1, "Year2": y2, "Rank": rank+1, "Word": word, "Diff": round(diff*100, 3), "Sign": "+" if diff > 0 else "-" if diff < 0 else "="}
            for rank, (word, diff) in enumerate(y2_diffs)
        ]

        results.append({
            "pair": f"{y1}-{y1+5} to {y2}-{y2+5}",
            "biggest_decrease": decrease_table,
            "biggest_increase": increase_table
        })

    return results


def count_treatments(t_types, tokens):
    terms = {"Corticosteroids" : ["corticosteroids"], "Immunosuppresants" : ["methotrexate", "azathioprine", "mycophenlate mofetil", "cyclosporine"],
             "Biologic Agents" : ["rituximab"], "Radiotherapy" : ["radiotherapy"], "Surgical Intervention" : ["surgery", "surgical"]}
    for t, t_list in terms.items():
        if any([w in tokens for w in t_list]):
            t_types[t] += 1
```

## Parsing a PDF
Given the collection of PDFs that we want to analyze, let us now describe how each document is processed. We first use the PyMuPDF tool to extract the visible
text of a PDF. Then we normalize the text. Normalization in our case includes reducing strings to lowercase and resolving similar looking unicode characters to a common
character. It is necessary because of how text is represented in digital documents like PDFs. A word in a PDF is a sequence of characters, and there are many more 
unicode characters than one might think, even when considering English-only text. For example, [this chart](https://gist.github.com/StevenACoffman/a5f6f682d94e38ed804182dc2693ed4b) shows unicode characters that
appear similar to some common English character. Each entry in the "look-alike" column is a different unicode character than the "original letter."

After normalizing text, we tokenize it. Tokenization involves splitting text into its constituent parts, i.e. words. This is done more or less by looking for whitespace
characters, and considering the groups of characters they surround as individual words.

Next, we remove stopwords, which are uninformative but frequently occurring words such as articles ("the", "an", etc), prepositions, and so on. 
More specifically, we leverage a list of stopwords designed for [medical corpora](https://www.oocities.org/gumby9/physicians/advanced/stopwords.pdf
). In addition to common English words, this list contains entries such as "oz" and "significantly" that may be less common in
everyday speech but prevalent in medical literature. We augment these words with some frequently occurring uninformative words we noticed in our experiments. 
A full list can be provided on request.

Lastly, we lemmatize the words. Lemmatization involves reducing words to their "root" word. For example, both "are" and "being" can be reduced to "be" via certain
lemmatizers. We perform this step so that very similar words can be considered "the same" and their counts can be added. Lemmatizers are imperfect and some words that appear to have the same root can be missed.

```{python}
import fitz
from tqdm import tqdm
tokenizer = load_medllama_3()
t_types = {"Corticosteroids" : 0, "Immunosuppresants" : 0, "Biologic Agents" : 0, "Radiotherapy" : 0, "Surgical Intervention" : 0} 
vocab_by_year = {}
counts = []
results = []
for year, file_list in files.items():
  vocab = {}
  pub_count = {}
  for f in file_list:
    with fitz.open(f) as pdf:
      text = " ".join([page.get_text("text") for page in pdf])
      
    tokens = filter_text(text, tokenizer) 
    count_treatments(t_types, tokens)            
    this_doc = set([])
    for t in tokens:
      if t not in vocab:
        vocab[t] = 0
      vocab[t] += 1
      if t not in this_doc:
        this_doc.add(t)
        if t not in pub_count:
            pub_count[t] = 0
        pub_count[t] += 1
  vocab_by_year[year] = vocab
  top_50 = sorted(vocab.items(), key=lambda item: item[1], reverse=True)[:100]
  #TODO: Replace this with some pretty printing; we're in a Quarto doc!
  #with open(year_dir / "top_50.txt", "w") as f:
  #  f.write("Rank | Word | Num_Docs\n")
  #  f.write("---------------------------\n")
  #  for i, word in enumerate(top_50):
  #    f.write(f"{i:>2} | {word[0]:<15} | {pub_count[word[0]]}\n")
  #with open(year_dir / "treatment_types.pkl", "wb") as f:
  #  pickle.dump(t_types, f)
  tab = [
    {
      "Rank": i + 1,
      "Word": word,
      "Num_Docs": pub_count[word]
    }
    for i, (word, _) in enumerate(top_50)
  ]
  results.append({
    "pair": f"{year}-{year+5}",
    "top_50": tab,
  })
  
  #for s, count in top_50:
  #  print(f"'{s}': {" ".join([str(ord(char)) for char in s])}")
  
ojs_define(diff_data = count_differences(vocab_by_year))
ojs_define(top50 = results)
#print(diff_data)
```

## TODO
I can describe the counting procedure if it's unclear.

```{ojs}
import {Table} from "@observablehq/inputs"

viewof year = Inputs.select(
  top50.map(d => d.pair),
{ label: "Select Year", value: top50[0].pair}
)

selected_year = top50.find(d => d.pair === year)
md`### Top Words`
Table(selected_year.top_50)

viewof year_pair = Inputs.select(
  diff_data.map(d => d.pair),
  { label: "Select Year Pair", value: diff_data[0].pair }
)
selected = diff_data.find(d => d.pair === year_pair)
md`### Words with Biggest Decrease`
Table(selected.biggest_decrease)
md`### Words with Biggest Increase`
Table(selected.biggest_increase)

```
